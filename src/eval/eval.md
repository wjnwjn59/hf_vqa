# VQA Evaluation

## Overview

Calculate evaluation metrics (Accuracy, ANLS) for VQA predictions generated by inference models.

**For running inference**, see [../inference/inference.md](../inference/inference.md).

## Quick Start

```bash
# Run evaluation (Accuracy + ANLS)
python -m src.eval.calculate_scores
```

### View Results

```bash
cat scores_accuracy.txt
cat scores_anls.txt
```

## Metrics

### Accuracy
Exact match after normalization (case-insensitive, removes punctuation).

**Examples:**
```python
gt = "pinterest"
pred = "Pinterest."    # Match = 100%
pred = "facebook"      # Match = 0%

# Multiple ground truths - matches ANY answer
gt = ["pinterest", "pin board"]
pred = "pin board"     # Match = 100%
```

### ANLS (Average Normalized Levenshtein Similarity)
String similarity score with threshold ≥ 0.5 (below threshold counts as 0).

**Examples:**
```python
gt = "pinterest"
pred = "pinterst"      # ANLS = 0.88 (minor typo)
pred = "facebook"      # ANLS = 0.0 (too different)

# Multiple ground truths - returns MAX score
gt = ["pinterest", "pin board"]
pred = "pinterst"      # ANLS = max(0.88, 0.44) = 0.88
```


## Input Requirements

Prediction JSON files from `src/inference/results/*.json`:
```json
[
  {
    "image": "37313.jpeg",
    "question": "Which social platform has heavy female audience?",
    "answer": ["pinterest"],
    "predict": "Pinterest",
    "answer_type": ["single span"],
    "evidence": ["text"],
    "operation/reasoning": []
  }
]
```

**Field mapping for category analysis:**
- `answer_type` → "Answer type" category
- `evidence` → "Element" category
- `operation/reasoning` → "Operation" category

## Output Files

After evaluation, results are saved in `src/inference/results/`:

**Summary files:**
- `final_scores.json` - Overall scores per model
- `detailed_analysis.json` - Category-level breakdown
- `scores_accuracy.txt` - Tab-separated accuracy table
- `scores_anls.txt` - Tab-separated ANLS table

**Updated prediction files:**
Each prediction entry gains two new fields:
```json
{
  "image": "37313.jpeg",
  "question": "Which social platform has heavy female audience?",
  "answer": ["pinterest"],
  "predict": "Pinterest",
  "answer_type": ["single span"],
  "evidence": ["text"],
  "operation/reasoning": [],
  "anls": 1.0,
  "accuracy": 1
}
```

**Tab-separated format example:**
```
Model	Overall	Single Span	Multi Span	Text	Table	List
InternVL3_5-8B	45.23	48.5	42.1	46.8	43.2	44.0
Qwen2.5-VL-7B	43.12	46.3	40.5	45.1	41.8	42.5
```

## Customization

### Change ANLS Threshold
Edit `src/eval/calculate_scores.py`:
```python
def compute_anls(gt: str, predict: str, threshold: float = 0.5):
```

## Project Structure

```
src/eval/
└── calculate_scores.py    # Score calculation & category analysis
```

