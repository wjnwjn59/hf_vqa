# VQA Evaluation

## Overview

Calculate evaluation metrics (Accuracy, ANLS, LLM Score) for VQA predictions generated by inference models.

**For running inference**, see [../inference/inference.md](../inference/inference.md).

## Quick Start

```bash
# Fast evaluation (Accuracy + ANLS only)
python -m src.eval.calculate_scores --no-llm-scores

# Full evaluation (includes LLM semantic scoring)
CUDA_VISIBLE_DEVICES=1 python -m src.eval.calculate_scores --llm-scores
```

### View Results

```bash
cat scores_accuracy.txt
cat scores_anls.txt
cat scores_llm.txt  # if --llm-scores was used
```

## Metrics

### Accuracy
Exact match after normalization (case-insensitive, removes punctuation).

**Examples:**
```python
gt = "pinterest"
pred = "Pinterest."    # Match = 100%
pred = "facebook"      # Match = 0%

# Multiple ground truths - matches ANY answer
gt = ["pinterest", "pin board"]
pred = "pin board"     # Match = 100%
```

### ANLS (Average Normalized Levenshtein Similarity)
String similarity score with threshold ≥ 0.5 (below threshold counts as 0).

**Examples:**
```python
gt = "pinterest"
pred = "pinterst"      # ANLS = 0.88 (minor typo)
pred = "facebook"      # ANLS = 0.0 (too different)

# Multiple ground truths - returns MAX score
gt = ["pinterest", "pin board"]
pred = "pinterst"      # ANLS = max(0.88, 0.44) = 0.88
```

### LLM Score (Optional)
Semantic evaluation using ensemble of LLM judges:
- Babel-9B-Chat
- Qwen3-8B
- InternLM3-8B-Instruct

Returns averaged score (0.0 to 1.0) across all judges.

**Examples:**
```python
Question: "How many people?"
GT: "5"
Pred: "there are 5 people"  # LLM Score = 1.0 (semantically correct)
Pred: "3 people"             # LLM Score = 0.0 (wrong answer)

# Multiple ground truths - uses first element for LLM prompt
GT: ["pinterest", "pin board"]  # Evaluates against "pinterest"
```

## Input Requirements

Prediction JSON files from `src/inference/results/*.json`:
```json
[
  {
    "image": "37313.jpeg",
    "question": "Which social platform has heavy female audience?",
    "answer": ["pinterest"],
    "predict": "Pinterest",
    "answer_type": ["single span"],
    "evidence": ["text"],
    "operation/reasoning": []
  }
]
```

**Field mapping for category analysis:**
- `answer_type` → "Answer type" category
- `evidence` → "Element" category
- `operation/reasoning` → "Operation" category

## Output Files

After evaluation, results are saved in `src/inference/results/`:

**Summary files:**
- `final_scores.json` - Overall scores per model
- `detailed_analysis.json` - Category-level breakdown
- `scores_accuracy.txt` - Tab-separated accuracy table
- `scores_anls.txt` - Tab-separated ANLS table
- `scores_llm.txt` - Tab-separated LLM score table

**Updated prediction files:**
Each prediction entry gains three new fields:
```json
{
  "image": "37313.jpeg",
  "question": "Which social platform has heavy female audience?",
  "answer": ["pinterest"],
  "predict": "Pinterest",
  "answer_type": ["single span"],
  "evidence": ["text"],
  "operation/reasoning": [],
  "anls": 1.0,
  "accuracy": 1,
  "llm_score": 0.0
}
```

**Tab-separated format example:**
```
Model	Overall	Single Span	Multi Span	Text	Table	List
InternVL3_5-8B	45.23	48.5	42.1	46.8	43.2	44.0
Qwen2.5-VL-7B	43.12	46.3	40.5	45.1	41.8	42.5
```

## Customization

### Change ANLS Threshold
Edit `src/eval/calculate_scores.py`:
```python
def compute_anls(gt: str, predict: str, threshold: float = 0.5):
```

### Change LLM Judges
Edit `src/eval/calculate_scores.py`:
```python
llms = [
    "/path/to/your/llm1",
    "/path/to/your/llm2",
]
```

### Adjust Batch Size
Edit `src/eval/calculate_scores.py`:
```python
batch_size = 32  # Adjust based on GPU memory
```

## Project Structure

```
src/eval/
└── calculate_scores.py    # Score calculation & category analysis
```

